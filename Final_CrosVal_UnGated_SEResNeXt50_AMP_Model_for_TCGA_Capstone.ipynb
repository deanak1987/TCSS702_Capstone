{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGY0X_DwWuFz"
   },
   "source": [
    "# TCGA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-V7ubdYW1Fs"
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "Connects to Google Drive\n",
    "Installs and imports dependancies\n",
    "Run whether starting or returning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6XKvnkQO1c1"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sys import platform\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import random \n",
    "from PIL import Image\n",
    "plt.style.use(\"ggplot\")\n",
    "import gc\n",
    "# import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from IPython.display import clear_output\n",
    "import torch.nn.utils.prune as prune\n",
    "import time\n",
    "import cv2\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gq5pcvTmKJ0b",
    "outputId": "f1d3209b-c5b9-4d23-accb-92c641bc7e6f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "process_count = len(os.sched_getaffinity(0))\n",
    "print('Processors available: ', process_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cyq9ewe-BJ1E",
    "outputId": "d12dce58-8428-48e0-8295-63db07483f77",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DjMfcdiPPuu"
   },
   "source": [
    "## Working with tiles\n",
    "\n",
    "Continue from here if resuming work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "id": "VnX9NSYTNkp_",
    "outputId": "f4174b99-aa10-4007-e67a-ee5e35f3290d"
   },
   "outputs": [],
   "source": [
    "%cd CancerPrediction\n",
    "df_reduced = pd.read_csv('slides/procesed_model_labels.csv')\n",
    "label = 'HER_Label'\n",
    "print(len(df_reduced))\n",
    "df_reduced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = []\n",
    "df = df_reduced\n",
    "for idx, row in df.iterrows():\n",
    "    patient_id.append(row.filename[0:12])\n",
    "df['patient_id'] = patient_id\n",
    "patient_count = df.patient_id.nunique()\n",
    "print(f'{len(df)} WSIs from {patient_count} patients.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get average dataset pixel value image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# avg_pix_val = np.zeros([256, 256, 3])\n",
    "# img_count = 0\n",
    "# tic = time.time()\n",
    "# count = 1\n",
    "# # for index, row in df_datalabels.iterrows():\n",
    "# for i in range(200):\n",
    "#     df = pd.read_csv(df_reduced.loc[i].Tile_DF)\n",
    "#     for idx, r in df.iterrows():\n",
    "#         tile = r.TilePath\n",
    "#         img = Image.open(tile).convert('RGB')\n",
    "#         if (img.size[0] == 256 and img.size[1] == 256):\n",
    "#             pix_val = np.asarray(img)\n",
    "#             avg_pix_val += pix_val\n",
    "#             img_count += 1\n",
    "#     if count % 10 == 0:\n",
    "#         print(f'{count} sets processed in {round((time.time() - tic) / 60, 4)} minutes.')\n",
    "#     count += 1\n",
    "# avg_pix_val = avg_pix_val / np.array([img_count])\n",
    "# avg_pix_val = np.around(avg_pix_val,0).astype(np.uint8)\n",
    "# pavg = np.around(np.average(avg_pix_val, axis = (0,1)),0).astype(np.uint8)\n",
    "\n",
    "# # avg_pix_val.shape\n",
    "# transform = T.Compose([T.ToTensor(),\n",
    "#                        # T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "#                        # T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#                        # T.RandomErasing(p=1, scale=(1,1), ratio=(1, 1), value=(pavg[0], pavg[1], pavg[2]), inplace=False),\n",
    "#                        T.ToPILImage()])\n",
    "# # apply the transform on image\n",
    "# avg_img = transform(avg_pix_val)\n",
    "\n",
    "# # avg_img = Image.fromarray(avg_img)\n",
    "# avg_img.save('avg_data_image1.jpg')\n",
    "# avg_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# import time\n",
    "\n",
    "# def process_image(start, end, shared_result):\n",
    "#     local_avg = 0\n",
    "#     for i in range(start, end):\n",
    "#         # Perform your computation here and add it to local_result\n",
    "#         df = pd.read_csv(df_reduced.iloc[i].Tile_DF)\n",
    "#         for idx, r in df.iterrows():\n",
    "#             img = Image.open(r.TilePath).convert('RGB')\n",
    "#             # Open and process the image\n",
    "#             # Split the image into its color channels\n",
    "#             red_channel, green_channel, blue_channel = img.split()\n",
    "\n",
    "#             # Calculate the sum of pixel values for each channel\n",
    "#             sum_red = sum(red_channel.getdata())\n",
    "#             sum_green = sum(green_channel.getdata())\n",
    "#             sum_blue = sum(blue_channel.getdata())\n",
    "\n",
    "#             # Add the sums to the shared result\n",
    "#             with shared_result.get_lock():\n",
    "#                 shared_result[0] += sum_red\n",
    "#                 shared_result[1] += sum_green\n",
    "#                 shared_result[2] += sum_blue\n",
    "\n",
    "\n",
    "# num_processes = multiprocessing.cpu_count()  # Number of CPU cores\n",
    "# print(num_processes)\n",
    "# num_iterations = len(df_reduced)\n",
    "# # Initialize a shared result for R, G, and B channels\n",
    "# shared_result = multiprocessing.Array('l', [0, 0, 0])\n",
    "\n",
    "# # Split the work into chunks for each process\n",
    "# chunk_size = len(df_reduced) // num_processes\n",
    "# processes = []\n",
    "# tic = time.time()\n",
    "# for i in range(num_processes):\n",
    "#     start = i * chunk_size\n",
    "#     end = (i + 1) * chunk_size if i < num_processes - 1 else len(df_reduced)\n",
    "#     p = multiprocessing.Process(target=process_image, args=(start, end, shared_result))\n",
    "#     processes.append(p)\n",
    "#     p.start()\n",
    "\n",
    "# # Wait for all processes to complete\n",
    "# for p in processes:\n",
    "#     p.join()\n",
    "\n",
    "# # Count number of tiles in dataset\n",
    "# num_images = 0\n",
    "# for ind, row in df_reduced.iterrows():\n",
    "#     num_images += len(pd.read_csv(row.Tile_DF))\n",
    "# print(num_images)\n",
    "\n",
    "# # Calculate the overall average pixel values for R, G, and B channels\n",
    "# total_red, total_green, total_blue = shared_result\n",
    "# average_red = total_red / (num_images * chunk_size) / (256 * 256)\n",
    "# average_green = total_green / (num_images * chunk_size) / (256 * 256)\n",
    "# average_blue = total_blue / (num_images * chunk_size) / (256 * 256)\n",
    "\n",
    "# print(f\"Average Red: {average_red}\")\n",
    "# print(f\"Average Green: {average_green}\")\n",
    "# print(f\"Average Blue: {average_blue}\")\n",
    "\n",
    "# # Example RGB channels as NumPy arrays (replace with your own data)\n",
    "# red_channel = np.full((256, 256), average_red, dtype=np.uint8)\n",
    "# green_channel = np.full((256, 256), average_green, dtype=np.uint8)\n",
    "# blue_channel = np.full((256, 256), average_blue, dtype=np.uint8)\n",
    "\n",
    "# # Stack the channels to create a single RGB image\n",
    "# rgb_image = np.stack((red_channel, green_channel, blue_channel), axis=-1)\n",
    "\n",
    "# # Convert the NumPy array to a PIL image\n",
    "# image = Image.fromarray(rgb_image)\n",
    "# print(f'Time to process: {round((time.time() - tic) / 60, 3)} minutes.)\n",
    "# # Save or display the image\n",
    "# image.save('output_image.jpg')\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# img = Image.open('avg_data_image.jpg')\n",
    "# pavg = np.around(np.average(img, axis = (0,1)),0).astype(np.uint8)\n",
    "# print(pavg)\n",
    "# transform = T.Compose([T.ToTensor(),\n",
    "#                        T.RandomErasing(p=1, scale=(1,1), ratio=(1, 1), value=(pavg[0], pavg[1], pavg[2]), inplace=False),\n",
    "#                        T.ToPILImage()])\n",
    "# # apply the transform on image\n",
    "# img = transform(img)\n",
    "# img.save('avg_data_image.jpg')\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for index, row in df_reduced.iterrows():\n",
    "#     print(f'{row.id}: {index + 1}/{len(df_reduced)}')\n",
    "#     remove = []\n",
    "#     df = pd.read_csv(row.Tile_DF)\n",
    "#     for idx, r in df.iterrows():\n",
    "#         tile = r.TilePath\n",
    "#         img = Image.open(tile).convert('RGB')\n",
    "#         if img.size[0] != 256 or img.size[1] != 256:\n",
    "#             remove.append(r.TilePath)\n",
    "#             print(f'{r.TilePath}: {img.size}')\n",
    "#             os.remove(r.TilePath)\n",
    "#     if len(remove) > 0:\n",
    "#         df = df[~df.TilePath.isin(remove)]\n",
    "#         df.to_csv(row.Tile_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training/validation/test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df_reduced[~df_reduced[label].isin([2])]\n",
    "######  ONLY UPDATE IF YOU WANT TO CHANGE CLASSIFICATION LABEL ######\n",
    "# df_train, df_test = train_test_split(df_reduced, test_size=0.2, stratify=df_reduced[label])\n",
    "\n",
    "# folders = ['train', 'test']\n",
    "# dataframes = [df_train, df_test]\n",
    "# for folder in folders:\n",
    "#     dataframes[folders.index(folder)].index = range(len(dataframes[folders.index(folder)].index))\n",
    "#     dataframes[folders.index(folder)] = dataframes[folders.index(folder)].loc[:, ~dataframes[folders.index(folder)].columns.str.contains(dataframes[folders.index(folder)].columns.tolist()[0])]\n",
    "#     dataframes[folders.index(folder)].to_csv(f'df_{folder}_{label}.csv')\n",
    "\n",
    "# df_train = dataframes[0]\n",
    "# df_test = dataframes[1]\n",
    "df_train = pd.read_csv(f'df_train_{label}.csv')\n",
    "df_test = pd.read_csv(f'df_test_{label}.csv')\n",
    "df_reduced[label].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPh7uUq0ggY7"
   },
   "source": [
    "## MIL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci4rEW-kgnJC"
   },
   "source": [
    "### Preview tile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "EUhg0trSYG7F",
    "outputId": "f705f607-4aee-45a4-ee49-1f81213675ca"
   },
   "outputs": [],
   "source": [
    "def is_blurry(image, threshold=100):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate the Laplacian of the image\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "    # Check if the variance is below the threshold\n",
    "    return laplacian_var < threshold\n",
    "\n",
    "# bag_index = 867\n",
    "bag_index =  random.randint(0, len(df_reduced))\n",
    "# bag_index = df_reduced.index[df_reduced.id == '4aa6bbd5-7b99-4261-bbb7-6372b78a5aa9'].tolist()[0]\n",
    "# bag_index\n",
    "tile_df = pd.read_csv(df_reduced.iloc[bag_index]['Tile_DF'])\n",
    "tile_index = random.randint(0, len(tile_df) - 1)\n",
    "# tile_index = 50\n",
    "print(f'WSI index:{bag_index}, WSi ID: {df_reduced.iloc[bag_index].id}, Label: {df_reduced.iloc[bag_index][label]}, Number of tiles: {len(tile_df)}, Tile index shown: {tile_index}')\n",
    "tile = tile_df.iloc[tile_index]['TilePath']\n",
    "img = Image.open(tile).convert('RGB')\n",
    "\n",
    "\n",
    "angles = [0, 90, 180, 270]\n",
    "degrees= angles[random.randrange(len(angles))]\n",
    "transform = T.Compose([T.ToTensor(),\n",
    "                       # T.RandomHorizontalFlip(),\n",
    "                       # T.RandomRotation((degrees,degrees)),\n",
    "                       # T.RandomAdjustSharpness(2, p=0.5),\n",
    "                       # T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "                       # T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                       # T.RandomErasing(p=1, scale=(0.39, 0.39), ratio=(1, 1), value=0, inplace=False),\n",
    "                       T.ToPILImage()])\n",
    "# apply the transform on image\n",
    "img = transform(img)\n",
    "img_arr = np.array(img)\n",
    "\n",
    "# Check if the image is blurry\n",
    "if is_blurry(img_arr):\n",
    "    print(\"The image is blurry.\")\n",
    "else:\n",
    "    print(\"The image is not blurry.\")\n",
    "\n",
    "# display the output image\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7caEZ1YjZhJ"
   },
   "source": [
    "## PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVgKvkHQjh_P"
   },
   "source": [
    "### Bag Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wd17IA0__R5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "class MILDataset(Dataset):\n",
    "    def __init__(self, dataframe, n_tiles, transformation, test):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transformation\n",
    "        self.n_tiles = n_tiles\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        bag_filepath = self.dataframe.iloc[index]['Tile_DF']\n",
    "        if not self.test:\n",
    "            bag_label = self.dataframe.iloc[index][label]\n",
    "        df_tile_list = []\n",
    "        # Load bag of tiles\n",
    "        df_tiles = pd.read_csv(bag_filepath)\n",
    "        bag_of_tiles = []\n",
    "        \n",
    "        max_n = len(df_tiles)\n",
    "        if self.n_tiles > max_n:\n",
    "            n_tiles = max_n\n",
    "        else:\n",
    "            n_tiles = self.n_tiles\n",
    "        tile_index = np.random.choice(len(df_tiles), n_tiles, replace=False)\n",
    "        for i in range(n_tiles):\n",
    "            # Load image using PIL\n",
    "            ran = random.uniform(0, 1)\n",
    "            if ran <= 1 or self.transform == test_transform:\n",
    "                image = Image.open(df_tiles.loc[tile_index[i]]['TilePath']).convert('RGB')\n",
    "                df_tile_list.append(df_tiles.loc[tile_index[i]].tolist())\n",
    "            else:\n",
    "                image = Image.open('avg_data_image.jpg')\n",
    "            # Apply transformations if provided\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            bag_of_tiles.append(image)\n",
    "\n",
    "        bag_of_tiles = torch.stack(bag_of_tiles)\n",
    "\n",
    "        if self.test:\n",
    "            return bag_of_tiles\n",
    "        else:\n",
    "            return bag_of_tiles, bag_label, df_tile_list\n",
    "\n",
    "\n",
    "angles = [0, 90, 180, 270]\n",
    "\n",
    "degrees= angles[random.randrange(len(angles))]\n",
    "train_transform = T.Compose([T.ToTensor(),\n",
    "                       T.RandomHorizontalFlip(),\n",
    "                       T.RandomRotation((degrees,degrees)),\n",
    "                       T.RandomVerticalFlip(),\n",
    "                       T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "                       T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                       T.RandomErasing(p=1, scale=(0.39, 0.39), ratio=(1, 1), value=0, inplace=False),\n",
    "                       ])\n",
    "\n",
    "test_transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 2                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CLcMR1LkTl8"
   },
   "source": [
    "### PyTorch Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1VzZvWnRIt4"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet18, resnet50, resnet101, resnet152, ResNet18_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights, resnext50_32x4d, ResNeXt50_32X4D_Weights, resnext101_64x4d, ResNeXt101_64X4D_Weights\n",
    "from torchvision.models import DenseNet121_Weights, DenseNet169_Weights, DenseNet201_Weights\n",
    "import timm\n",
    "\n",
    "class MILModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MILModel, self).__init__()\n",
    "        # Define your model architecture here\n",
    "        self.L1 = 1024 # 1024 node fully connected layer\n",
    "        self.L = 512 # 512 node fully connected layer\n",
    "        self.D = 128 # 128 node attention layer\n",
    "        self.K = 1\n",
    "        self.resnet_list = [[resnet18(weights=ResNet18_Weights.IMAGENET1K_V1), 'Resnet18'],\n",
    "                            # [resnet50(weights=ResNet50_Weights.IMAGENET1K_V2), 'ResNet50'],\n",
    "                            # [resnet101(weights=ResNet101_Weights.IMAGENET1K_V2), 'ResNet101'],\n",
    "                            # [resnet152(weights=ResNet152_Weights.IMAGENET1K_V2), 'ResNet152'],\n",
    "                            # [resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V2), 'ResNeXt50_32x4d'],\n",
    "                            # [resnext101_64x4d(weights=ResNeXt101_64X4D_Weights.IMAGENET1K_V1), 'ResNeXt101_64x4d'],\n",
    "                            # [torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d'), 'SE_ResNext101_32x4d'],\n",
    "                            # [timm.create_model('seresnextaa101d_32x8d.sw_in12k_ft_in1k_288', pretrained=True), 'SE_ResNext101_32x8d'],\n",
    "                            [timm.create_model('seresnext50_32x4d', pretrained=True),'SE_ResNeXt50_32x4d']\n",
    "                            # [torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d'), 'SE_ResNeXt101_32x4d']\n",
    "                        ]\n",
    "        self.densenet_list = [#[torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', weights=DenseNet121_Weights.IMAGENET1K_V1), 'DenseNet121'],\n",
    "                            # [torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', weights=DenseNet169_Weights.IMAGENET1K_V1), 'DenseNet169'],\n",
    "                            [torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', weights=DenseNet201_Weights.IMAGENET1K_V1), 'DenseNet201']\n",
    "                            ]\n",
    "        \n",
    "        #Choose model\n",
    "        self.choice = self.resnet_list[1]\n",
    "        # self.choice = self.densenet_list[0]\n",
    "        self.model = self.choice[0]\n",
    "        self.name = self.choice[1]        # self.model = self.densenet_list[0]\n",
    "        \n",
    "        if self.choice in self.resnet_list:\n",
    "            self.num_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Sequential(\n",
    "                nn.Identity(),\n",
    "                nn.Linear(self.num_features, self.L1),\n",
    "                nn.Dropout(0.5, False),\n",
    "                nn.Linear(self.L1, self.L),\n",
    "                nn.Dropout(0.5, False)\n",
    "            )\n",
    "            nn.init.kaiming_normal_(self.model.fc[1].weight, mode='fan_in', nonlinearity='relu')\n",
    "            nn.init.kaiming_normal_(self.model.fc[3].weight, mode='fan_in', nonlinearity='relu')\n",
    "            \n",
    "        elif self.choice in self.densenet_list:\n",
    "            self.num_features = self.model.classifier.in_features\n",
    "            # self.model.classifier = nn.Linear(self.num_features, 1)\n",
    "            self.pool = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.linear_layers = nn.Sequential(\n",
    "                nn.Identity(),\n",
    "                # nn.Flatten(),\n",
    "                nn.Linear(self.num_features, self.L1),\n",
    "                nn.Dropout(0.5, False),\n",
    "                nn.Linear(self.L1, self.L),\n",
    "                nn.Dropout(0.5, False)\n",
    "            )\n",
    "            nn.init.kaiming_normal_(self.linear_layers[1].weight, mode='fan_in', nonlinearity='relu')\n",
    "            nn.init.kaiming_normal_(self.linear_layers[3].weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        self.drop = nn.Dropout(0.5, False)\n",
    "   \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "        self.classifierBCE = nn.Sequential(\n",
    "            nn.Linear(self.L, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.classifierBCEL = nn.Sequential(\n",
    "            nn.Linear(self.L, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, bag):\n",
    "        # Compute the representation of bags_of_tiles\n",
    "        # Process each bag of tiles to get a single representation\n",
    "        if self.choice in self.resnet_list:\n",
    "            x = self.model(bag)\n",
    "            \n",
    "        elif self.choice in self.densenet_list:\n",
    "            x = self.model.features(bag)\n",
    "            x = self.pool(x)\n",
    "            x = self.linear_layers(x)\n",
    "                           \n",
    "\n",
    "        A = self.attention(x) # NxK\n",
    "        A = torch.transpose(A, 1, 0) # KxN\n",
    "        A = F.softmax(A, dim=1) # softmax over N\n",
    "        M = torch.mm(A, x)\n",
    "        if criterion == nn.BCELoss:\n",
    "            out = self.classifierBCE(M)\n",
    "        else:\n",
    "            out = self.classifierBCEL(M)        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use AMP for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tic = time.time()\n",
    "    pred_array = []\n",
    "    label_array = []\n",
    "    print(f'Epoch: {epoch}/{num_epochs}')\n",
    "    loop = tqdm(train_loader)\n",
    "    for idx, batch in enumerate(loop):\n",
    "        data, label, df_tiles = batch\n",
    "        label_array.append(label[0].item())\n",
    "        data = torch.squeeze(data)\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # calculate, loss, and predictions\n",
    "        # Forward pass\n",
    "        with autocast():  # Enables automatic mixed precision\n",
    "            out = model.forward(data)\n",
    "            if criterion == nn.BCELoss:\n",
    "                y_prob = out\n",
    "                loss = criterion(torch.squeeze(y_prob,1).float(), label.float())\n",
    "                if label.item() > 0 and y_prob.item() < threshold:\n",
    "                    loss += criterion(torch.squeeze(y_prob,1).float(), label.float())\n",
    "            else: \n",
    "                loss = criterion(torch.squeeze(out,1).float(), label.float())\n",
    "                # loss = criterion(np.asarray([out.item()]), label.float())\n",
    "                y_prob = torch.sigmoid(out)\n",
    "                if class1 < class0:\n",
    "                    if label.item() > 0 and y_prob.item() < threshold:\n",
    "                        loss += criterion(torch.squeeze(out,1).float(), label.float())\n",
    "                else:\n",
    "                    if label.item() < 0 and y_prob.item() > threshold:\n",
    "                        loss += criterion(torch.squeeze(out,1).float(), label.float())\n",
    "                        \n",
    "        # y_prob, y_hat = model.forward(data)\n",
    "        y_hat = torch.ge(y_prob, threshold).float()\n",
    "        pred_array.append(y_hat.item())\n",
    "        total += 1\n",
    "        if y_hat == label:\n",
    "            correct += 1\n",
    "        train_loss += loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        \n",
    "    toc = time.time()\n",
    "    \n",
    "    roc_auc = round(roc_auc_score(np.asarray(label_array), np.asarray(pred_array)), 4)\n",
    "    # calculate loss and error for epoch\n",
    "    total_loss = round(train_loss.cpu().float().data.item() / len(train_loader), 4)\n",
    "    accuracy =  round(correct / total, 4)\n",
    "    print(f'Train Set, Avg. Loss: {total_loss}, Accuracy: {accuracy}, AUROC: {roc_auc}, Train time: {round(toc-tic)}s')\n",
    "   \n",
    "    return total_loss, accuracy, roc_auc\n",
    "\n",
    "def validate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    label_array = []\n",
    "    pred_array = []\n",
    "    prob_array = []\n",
    "    with torch.no_grad():\n",
    "        tic = time.time()\n",
    "        loop = tqdm(test_loader)\n",
    "        for idx, batch in enumerate(loop):\n",
    "            data, label, df_tiles = batch\n",
    "            data = torch.squeeze(data)\n",
    "            label_array.append(label.item())\n",
    "            data, label = Variable(data), Variable(label)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # calculate, loss, and predictions\n",
    "            with autocast():  # Enables automatic mixed precision\n",
    "                out = model.forward(data)\n",
    "                if criterion == nn.BCELoss:\n",
    "                    y_prob = out\n",
    "                    loss = criterion(torch.squeeze(y_prob,1).float(), label.float())\n",
    "                else: \n",
    "                    loss = criterion(torch.squeeze(out,1).float(), label.float())\n",
    "                    y_prob = torch.sigmoid(out)\n",
    "            total += 1\n",
    "            y_hat = torch.ge(y_prob, threshold).float()\n",
    "            if y_hat == label:\n",
    "                correct += 1\n",
    "            pred_array.append(y_hat.item())\n",
    "            prob_array.append(y_prob.item())\n",
    "            test_loss += loss\n",
    "            \n",
    "        toc = time.time()\n",
    "        roc_auc = round(roc_auc_score(np.asarray(label_array), np.asarray(pred_array), average='micro'), 4)\n",
    "        # calculate loss and error for epoch\n",
    "        total_loss = round(test_loss.cpu().float().data.item() / len(test_loader), 4)\n",
    "        accuracy =  round(correct / total, 4)\n",
    "        print(f'Avg. Loss: {total_loss}, Accuracy: {accuracy}, AUROC: {roc_auc}, Val time: {round(toc-tic)}s')\n",
    "        return total_loss, accuracy, roc_auc, pred_array, label_array, prob_array\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    pred_array = []\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(test_loader)\n",
    "        for idx, batch in enumerate(loop):\n",
    "            data = Variable(torch.squeeze(batch))\n",
    "            data = data.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # calculate, loss, and predictions\n",
    "            with autocast():  # Enables automatic mixed precision\n",
    "                out = model.forward(data)\n",
    "                if criterion == nn.BCELoss:\n",
    "                    y_prob = out\n",
    "                else: \n",
    "                    y_prob = torch.sigmoid(out)\n",
    "            y_hat = torch.ge(y_prob, threshold).float()\n",
    "            pred_array.append(y_hat.item())\n",
    "            \n",
    "        \n",
    "        return pred_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal worker number for data loader. UNCOMMENT FOR USE IN NEW MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time\n",
    "# import multiprocessing as mp\n",
    "# df_1 = c1.sample(s_num, replace=replace)\n",
    "# df_0 = c0.sample(s_num, replace=replace)\n",
    "# df_epoch_train = pd.concat([df_1, df_0], axis=0)\n",
    "# df_epoch_train = df_epoch_train.reset_index(drop=True)\n",
    "# n_tiles = 50\n",
    "# workers = pd.DataFrame(columns=['workers', 'time'])\n",
    "# train_dataset = MILDataset(df_epoch_train, n_tiles, train_transform, False)\n",
    "# for num_workers in range(12, 22,2):#mp.cpu_count(), 2):  \n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True, num_workers=num_workers)\n",
    "#     start = time()\n",
    "#     for epoch in range(1, 3):\n",
    "#         for i, data in enumerate(train_dataloader, 0):\n",
    "#             pass\n",
    "#     end = time()\n",
    "#     workers.loc[len(workers)] = [num_workers, end - start]\n",
    "#     print(f\"Finished with:{round(end - start, 2)} seconds, num_workers={num_workers}\")\n",
    "# workers.to_csv('num_workers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iinitiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definitions ###\n",
    "def accuracy(predictions, labels):\n",
    "    count = 0\n",
    "    for i in range(len(labels)):\n",
    "        if predictions[i] == labels[i]:\n",
    "            count += 1\n",
    "    return count / len(labels)\n",
    "\n",
    "def test_metrics(actual, df, choice, type, probs):\n",
    "    if choice == 'max':\n",
    "        df['Prediction'] = df.max(axis=1)   \n",
    "    elif choice == 'min':\n",
    "        df['Prediction'] = df.min(axis=1)   \n",
    "    else:\n",
    "        df['avg'] = df.sum(axis=1) / df.count(axis='columns')\n",
    "        df['Prediction'] = df.avg.agg(lambda x: 1 if x >= 0.5 else 0)\n",
    "    df['label'] = actual\n",
    "    df['probs'] = probs\n",
    "    df.to_csv(f'{type}_{choice}_predictions.csv')\n",
    "    predictions = df.Prediction.tolist()\n",
    "    e_acc = round(accuracy(predictions, actual),4)\n",
    "    e_roc_auc = round(roc_auc_score(np.asarray(actual), np.asarray(predictions), average='micro'), 4)\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(actual, predictions)\n",
    "    # Extract values from confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensi = tp / (tp + fn)\n",
    "    # Calculate specificity\n",
    "    speci = tn / (tn + fp)\n",
    "\n",
    "    print(cm)\n",
    "    print(f'Test ensemble Accuracy: {e_acc}, AUROC: {e_roc_auc} using {choice} as the selector.')\n",
    "    return e_acc, e_roc_auc, sensi, speci\n",
    "\n",
    "def save_ckp(state, checkpoint_dir, is_best):\n",
    "    if is_best == True:\n",
    "        f_path = f'{checkpoint_dir}/best_checkpoint-{date}.pt'\n",
    "    else:\n",
    "        f_path = f'{checkpoint_dir}/checkpoint-{date}.pt'\n",
    "    torch.save(state, f_path)\n",
    "    \n",
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch'], checkpoint['statistics'], checkpoint['best_auc'], checkpoint['best_epoch'], checkpoint['ensemble_imp'], checkpoint['last_fold'], checkpoint['fold_score'], checkpoint['early_stopper']\n",
    "\n",
    "def create_checkpoint(epoch, model, optimizer, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, last_fold, fold_score, early_stopper):\n",
    "    checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'statistics': stats,\n",
    "            'best_auc': best_val_auc,\n",
    "            'best_epoch': best_val_auc_epoch,\n",
    "            'ensemble_imp': ensemble_improvement,\n",
    "            'last_fold': last_fold,\n",
    "            'fold_score': fold_score,\n",
    "            'early_stopper': early_stopper\n",
    "    }\n",
    "    return checkpoint\n",
    "\n",
    "def print_roll_avg(x, y):\n",
    "    blue_color = '\\033[94m'\n",
    "    reset_color = '\\033[0m'\n",
    "    print(f'Rolling Best Cross-validation Scores: [',end = '')\n",
    "    for i in x:\n",
    "        print(i, end=', ')\n",
    "    print(f'{blue_color}{y}{reset_color}]')\n",
    "    print(f'Rolling Best Cross-validation Average: {round((sum(x)+ y) / (len(x) + 1), 4)}\\n')\n",
    "\n",
    "\n",
    "class ClassBalancedLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=1.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        eps = 1e-12\n",
    "        count = target.float().sum(dim=0)\n",
    "        weight = (1 - count / count.sum()) ** self.gamma\n",
    "        weight = weight / weight.sum()\n",
    "        loss = weight * torch.nn.BCEWithLogitsLoss()(logits, target)\n",
    "        return loss\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.avg_t_loss = 0\n",
    "        self.avg_v_loss = 0\n",
    "        \n",
    "    def print_stop_status(self):\n",
    "        print(f'Early stop status: {self.counter}/{self.patience}')\n",
    "        print(f'Last 10 average Train Loss: {self.avg_t_loss}')\n",
    "        print(f'Last 10 average Val   Loss: {self.avg_v_loss}')\n",
    "        \n",
    "    def early_stop(self, v_loss, t_loss):\n",
    "        self.train_losses.append(t_loss)\n",
    "        self.val_losses.append(v_loss)\n",
    "        if len(self.train_losses) > 10:\n",
    "            del self.train_losses[0]\n",
    "            del self.val_losses[0]\n",
    "        if v_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = v_loss\n",
    "            self.counter = 0\n",
    "        if v_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            self.print_stop_status()\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        self.avg_t_loss = sum(self.train_losses) / len(self.train_losses)\n",
    "        self.avg_v_loss = sum(self.val_losses) / len(self.val_losses)\n",
    "        if (self.avg_v_loss - self.avg_t_loss) > 0.35 and len(self.train_losses) == 10 and epoch > 50:\n",
    "            return True\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Initializers\n",
    "checkpoint_dir = 'checkpoints' # Directory to save checkpoints to\n",
    "num_epochs = 300 # Number of epochs to train on\n",
    "n_tiles = 50\n",
    "n_tiles_val = 500\n",
    "date = '2-12' # Checkpoint date\n",
    "ckp_path = f\"checkpoints/checkpoint-{date}.pt\"\n",
    "e_choice = 'max' # How to select ensemble prediction\n",
    "is_best = False # Best model so far?\n",
    "restart = True # Restarting from a current epoch?\n",
    "num_vals = 1 # number of ensemble member runs\n",
    "threshold = 0.5 # y_hat probability threshold\n",
    "df_num_workers = pd.read_csv('num_workers.csv')\n",
    "num_workers = int(df_num_workers.workers.iloc[df_num_workers.time.idxmin()])\n",
    "\n",
    "# Class size determination\n",
    "if df_train[label].value_counts().index.tolist()[0] == 1:\n",
    "    class1, class0 = df_train[label].value_counts()\n",
    "else:\n",
    "    class0, class1 = df_train[label].value_counts()\n",
    "\n",
    "# Number of splits (k)\n",
    "k_folds = 5\n",
    "print(num_workers)\n",
    "########## Create the MIL model\n",
    "# model = MILGatedModel()\n",
    "model = MILModel()\n",
    "model.name += F'_{label}'\n",
    "\n",
    "########### Define the loss criterion\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = ClassBalancedLoss()\n",
    "\n",
    "# ############ Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, \n",
    "                             betas=(0.9, 0.999), \n",
    "                             eps=1e-08, \n",
    "                             weight_decay=5e-5)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "print(f'Running the model on {device}')\n",
    "model.to(device)\n",
    "print(f'Ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Cross-validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart from the previous checkpoint?\n",
    "if restart:\n",
    "    model, optimizer, start_epoch, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, last_fold, fold_score, early_stopper = load_ckp(ckp_path, model, optimizer)\n",
    "    model.to(device)\n",
    "    \n",
    "else: \n",
    "    start_epoch = 1\n",
    "    best_val_auc = 0\n",
    "    best_val_auc_epoch = 0\n",
    "    ensemble_improvement = []\n",
    "    stats = [[],[],[],[],[],[],[]] # [train_losses, train_accuracies, train_aucs, val_losses, val_accuracies, val_aucs, ensemble_auc] \n",
    "    fold_score = []\n",
    "    last_fold = 0\n",
    "    early_stopper = EarlyStopper(patience=10, min_delta=0.6)\n",
    "\n",
    "\n",
    "# Initialize KFold\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(df_train, df_train[label])):\n",
    "    clear_output(wait=True)\n",
    "    if restart == True:\n",
    "        print(f'Resuming {model.name} model from epoch: {start_epoch}')\n",
    "\n",
    "    if fold < last_fold:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            ############## Save data ############\n",
    "            checkpoint = create_checkpoint(epoch, model, optimizer, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, fold, fold_score, early_stopper)\n",
    "            save_ckp(checkpoint, checkpoint_dir, False)\n",
    "        except NameError:\n",
    "            pass\n",
    "        \n",
    "        print(f'Start Training with {model.name}')\n",
    "        df_cross_train = df_train.iloc[train_index]\n",
    "        df_cross_val = df_train.iloc[val_index]\n",
    "        if df_cross_train[label].value_counts().index.tolist()[0] == 1:\n",
    "            class1, class0 = df_cross_train[label].value_counts()\n",
    "        else:\n",
    "            class0, class1 = df_cross_train[label].value_counts()\n",
    "        c1 = df_cross_train[df_cross_train[label] == 1]\n",
    "        c0 = df_cross_train[df_cross_train[label] == 0]\n",
    "        sample = 'Und' # sampling type\n",
    "        s_num = 1\n",
    "        \n",
    "        if sample == 'Ov':\n",
    "             # Basic even sampling of training data using oversampling, based on the fewest label count\n",
    "            if class0 >= class1:\n",
    "                s_num = class0\n",
    "            else: \n",
    "                s_num = class1\n",
    "            replace = True  \n",
    "        else:\n",
    "            # Basic even sampling of training data using undersampling, based on the fewest label count\n",
    "            if class0 <= class1:\n",
    "                s_num = class0\n",
    "            else: \n",
    "                s_num = class1\n",
    "            replace = False\n",
    "        \n",
    "        df_1 = c1.sample(s_num, replace=replace)\n",
    "        df_0 = c0.sample(s_num, replace=replace)\n",
    "        df_epoch_train = pd.concat([df_1, df_0], axis=0)\n",
    "        df_epoch_train = df_epoch_train.reset_index(drop=True)\n",
    "        df_epoch_train[label].value_counts()   \n",
    "        \n",
    "        # Begin training\n",
    "        for epoch in range(start_epoch, num_epochs + 1):\n",
    "            print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "            print_roll_avg(fold_score, best_val_auc)\n",
    "            \n",
    "            print(f'{sample}ersampled Training Set')\n",
    "            \n",
    "            \n",
    "            if sample == 'Und':\n",
    "                df_1 = c1.sample(s_num, replace=replace)\n",
    "                df_0 = c0.sample(s_num, replace=replace)\n",
    "                df_epoch_train = pd.concat([df_1, df_0], axis=0)\n",
    "                df_epoch_train = df_epoch_train.reset_index(drop=True)\n",
    "            if epoch > 1:\n",
    "                print(f'Continuing training with {model.name}')\n",
    "            # # Build training dataloader\n",
    "            train_dataset = MILDataset(df_epoch_train, n_tiles, train_transform, False)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True, num_workers=num_workers)\n",
    "            # train model\n",
    "            train_loss, train_acc, train_auc = train(model, device, train_dataloader, optimizer, epoch)\n",
    "            print(torch.cuda.max_memory_allocated(device=None) / (1024 ** 2))\n",
    "            # Test the model on validation data\n",
    "            print(f'Running {num_vals} tests on validation data.')\n",
    "            losses = []\n",
    "            accs = []\n",
    "            aucs = []\n",
    "            labels = []\n",
    "            \n",
    "            df_trials = pd.DataFrame()\n",
    "            for run in range(num_vals): # Run several validations to account for random tile selections\n",
    "                # Build validation dataloader\n",
    "                val_dataset = MILDataset(df_cross_val, n_tiles_val, test_transform, False)    \n",
    "                val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "                val_loss, val_acc, val_auc, val_preds, val_labels, val_prob = validate(model, device, val_dataloader)\n",
    "                losses.append(val_loss)\n",
    "                accs.append(val_acc)\n",
    "                aucs.append(val_auc)\n",
    "                df_trials[run] = val_preds\n",
    "                \n",
    "            avg_val_loss = round(sum(losses) / len(losses), 4)\n",
    "            avg_val_acc = round(sum(accs) / len(accs), 4)\n",
    "            avg_val_auc = round(sum(aucs) / len(aucs), 4)\n",
    "            time.sleep(5)\n",
    "            if epoch < num_epochs:\n",
    "                clear_output(wait=True)\n",
    "            print(f'  Average validation scores: Loss = {avg_val_loss}, Acc = {avg_val_acc}, AUROC = {avg_val_auc}')\n",
    "\n",
    "            # Ensemble prediction\n",
    "            e_acc, e_roc_auc, sens, spec = test_metrics(df_cross_val[label].tolist(), df_trials, e_choice, 'validation', val_prob)\n",
    "            improved = 100*round(e_roc_auc - avg_val_auc,3)\n",
    "            ensemble_improvement.append(improved)\n",
    "            if avg_val_auc >= best_val_auc:\n",
    "                best_val_auc = avg_val_auc\n",
    "                best_val_auc_epoch = epoch\n",
    "                is_best = True\n",
    "                checkpoint = create_checkpoint(epoch, model, optimizer, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, fold, fold_score, early_stopper)\n",
    "                save_ckp(checkpoint, checkpoint_dir, is_best)\n",
    "            else:\n",
    "                 is_best = False   \n",
    "        \n",
    "            \n",
    "            avg_e_improv = round(sum(ensemble_improvement) / len(ensemble_improvement),3)\n",
    "        \n",
    "            print(f'Best average AUROC: {best_val_auc} at epoch: {best_val_auc_epoch} Average Ensemble Improvement: {avg_e_improv} points')\n",
    "        \n",
    "            stats[0].append(train_loss)\n",
    "            stats[1].append(train_acc)\n",
    "            stats[2].append(train_auc)\n",
    "            stats[3].append(avg_val_loss)\n",
    "            stats[4].append(avg_val_acc)\n",
    "            stats[5].append(avg_val_auc)\n",
    "            stats[6].append(e_roc_auc)\n",
    "\n",
    "            # Plot the results\n",
    "            figure, axis = plt.subplots(1, 3)\n",
    "            axis[0].plot(range(epoch), stats[0], label = 'train')\n",
    "            axis[0].plot(range(epoch), stats[3], label = 'val')\n",
    "            axis[0].set_title('Loss')\n",
    "            axis[0].set_xlabel('Epoch')\n",
    "            axis[1].plot(range(epoch), stats[1], label = 'train')\n",
    "            axis[1].plot(range(epoch), stats[4], label = 'val')\n",
    "            axis[1].set_title('Accuracy')\n",
    "            axis[1].set_xlabel('Epoch')\n",
    "            axis[2].plot(range(epoch), stats[2], label = 'train')\n",
    "            axis[2].plot(range(epoch), stats[5], label = 'val')\n",
    "            axis[2].plot(range(epoch), stats[6], label = 'ensemble')\n",
    "            axis[2].set_title('AUROC')\n",
    "            axis[2].set_xlabel('Epoch')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'figures/{model.name}_fold{fold+1}.png')\n",
    "            plt.show()\n",
    "\n",
    "            early_stopper.print_stop_status()\n",
    "\n",
    "            # Save metrics\n",
    "            print(f'Saving metrics...')\n",
    "            checkpoint = create_checkpoint(epoch + 1, model, optimizer, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, fold, fold_score, early_stopper)\n",
    "            save_ckp(checkpoint, checkpoint_dir, False)\n",
    "            if early_stopper.early_stop(avg_val_loss, train_loss):             \n",
    "                break\n",
    "            \n",
    "            print()\n",
    "       \n",
    "        fold_score.append(best_val_auc)\n",
    "        ########## Fold Complete reinitialize training model ###########\n",
    "        if fold < k_folds - 1:\n",
    "            model = MILModel()\n",
    "            model.name += F'_{label}'\n",
    "            \n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, \n",
    "                                     betas=(0.9, 0.999), \n",
    "                                     eps=1e-08, \n",
    "                                     weight_decay=5e-5)\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            model.to(device)\n",
    "            print(f'Ready to go!')\n",
    "            early_stopper = EarlyStopper(patience=10, min_delta=0.6)\n",
    "            start_epoch = 1\n",
    "            best_val_auc = 0\n",
    "            best_val_auc_epoch = 0\n",
    "            ensemble_improvement = []\n",
    "            stats = [[],[],[],[],[],[],[]] # [train_losses, train_accuracies, train_aucs, val_losses, val_accuracies, val_aucs, ensemble_auc] \n",
    "            epoch = start_epoch\n",
    "\n",
    "print('Cross-Validation complete!')\n",
    "print(fold_score)\n",
    "print(sum(fold_score) / len(fold_score))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckp(state, checkpoint_dir, is_best):\n",
    "    if is_best == True:\n",
    "        f_path = f'{checkpoint_dir}/best_checkpoint-{date}.pt'\n",
    "    else:\n",
    "        f_path = f'{checkpoint_dir}/checkpoint-{date}.pt'\n",
    "    torch.save(state, f_path)\n",
    "    \n",
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch'], checkpoint['statistics'], checkpoint['best_auc'], checkpoint['best_epoch'], checkpoint['ensemble_imp'], checkpoint['sens'], checkpoint['spec'], checkpoint['early_stopper']\n",
    "\n",
    "def create_checkpoint(epoch, model, optimizer, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, sens, spec, early_stopper):\n",
    "    checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'statistics': stats,\n",
    "            'best_auc': best_val_auc,\n",
    "            'best_epoch': best_val_auc_epoch,\n",
    "            'ensemble_imp': ensemble_improvement,\n",
    "            'sens': sens,\n",
    "            'spec': spec,\n",
    "            'early_stopper': early_stopper\n",
    "    }\n",
    "    return checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### RESET FOR NEW HRS ########\n",
    "# df_test_train, df_test_val = train_test_split(df_train, test_size=0.15, stratify=df_train[label])\n",
    "# df_test_train.to_csv(f'test_train_set_{label}.csv')\n",
    "# df_test_val.to_csv(f'test_val_set_{label}.csv')\n",
    "# ##################################\n",
    "\n",
    "df_test_train = pd.read_csv(f'test_train_set_{label}.csv')\n",
    "df_test_val = pd.read_csv(f'test_val_set_{label}.csv')\n",
    "######### Initializers\n",
    "checkpoint_dir = 'checkpoints' # Directory to save checkpoints to\n",
    "num_epochs = 300 # Number of epochs to train on\n",
    "n_tiles = 50\n",
    "n_tiles_val = 500\n",
    "date = '2-26' # Checkpoint date\n",
    "ckp_path = f\"checkpoints/checkpoint-{date}.pt\"\n",
    "is_best = False # Best model so far?\n",
    "restart = True # Restarting from a current epoch?\n",
    "num_vals = 1 # number of ensemble member runs\n",
    "threshold = 0.5 # y_hat probability threshold\n",
    "df_num_workers = pd.read_csv('num_workers.csv')\n",
    "num_workers = int(df_num_workers.workers.iloc[df_num_workers.time.idxmin()])\n",
    "\n",
    "model = MILModel()\n",
    "model.name += F'_{label}'\n",
    "\n",
    "########### Define the loss criterion\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ############ Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, \n",
    "                             betas=(0.9, 0.999), \n",
    "                             eps=1e-08, \n",
    "                             weight_decay=5e-5)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "print(f'Running the model on {device}')\n",
    "model.to(device)\n",
    "print(f'Ready to go!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Restart from the previous checkpoint?\n",
    "if restart:\n",
    "    model, optimizer, start_epoch, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, sens, spec, early_stopper = load_ckp(ckp_path, model, optimizer)\n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "else: \n",
    "    start_epoch = 1\n",
    "    best_val_auc = 0\n",
    "    best_val_auc_epoch = 0\n",
    "    ensemble_improvement = []\n",
    "    stats = [[],[],[],[],[],[],[]] # [train_losses, train_accuracies, train_aucs, val_losses, val_accuracies, val_aucs, ensemble_auc] \n",
    "    sens = 0\n",
    "    spec = 0\n",
    "    early_stopper = EarlyStopper(patience=10, min_delta=0.6)\n",
    "if restart == True:\n",
    "    print(f'Resuming {model.name} model from epoch: {start_epoch}')\n",
    "\n",
    "\n",
    "try:\n",
    "    ############## Save data ############\n",
    "    checkpoint = create_checkpoint(epoch, model, optimizer, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, sens, spec, early_stopper)\n",
    "    save_ckp(checkpoint, checkpoint_dir, False)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "print(f'Start Training with {model.name}')\n",
    "# print(f'Train set size: {len(df_cross_train)}, Val set size: {len(df_cross_val)}')\n",
    "if df_test_train[label].value_counts().index.tolist()[0] == 1:\n",
    "    class1, class0 = df_test_train[label].value_counts()\n",
    "else:\n",
    "    class0, class1 = df_test_train[label].value_counts()\n",
    "c1 = df_test_train[df_test_train[label] == 1]\n",
    "c0 = df_test_train[df_test_train[label] == 0]\n",
    "sample = 'Und' # sampling type\n",
    "s_num = 1\n",
    "\n",
    "if sample == 'Ov':\n",
    "     # Basic even sampling of training data using oversampling, based on the fewest label count\n",
    "    if class0 >= class1:\n",
    "        s_num = class0\n",
    "    else: \n",
    "        s_num = class1\n",
    "    replace = True  \n",
    "else:\n",
    "    # Basic even sampling of training data using undersampling, based on the fewest label count\n",
    "    if class0 <= class1:\n",
    "        s_num = class0\n",
    "    else: \n",
    "        s_num = class1\n",
    "    replace = False\n",
    "\n",
    "df_1 = c1.sample(s_num, replace=replace)\n",
    "df_0 = c0.sample(s_num, replace=replace)\n",
    "df_epoch_train = pd.concat([df_1, df_0], axis=0)\n",
    "df_epoch_train = df_epoch_train.reset_index(drop=True)\n",
    "df_epoch_train[label].value_counts()   \n",
    "\n",
    "# Begin training\n",
    "for epoch in range(start_epoch, num_epochs + 1):        \n",
    "    print(f'{sample}ersampled Training Set')\n",
    "    \n",
    "    \n",
    "    if sample == 'Und':\n",
    "        df_1 = c1.sample(s_num, replace=replace)\n",
    "        df_0 = c0.sample(s_num, replace=replace)\n",
    "        df_epoch_train = pd.concat([df_1, df_0], axis=0)\n",
    "        df_epoch_train = df_epoch_train.reset_index(drop=True)\n",
    "        # print(df_epoch_train[label].value_counts())\n",
    "    if epoch > 1:\n",
    "        print(f'Continuing training with {model.name}')\n",
    "    # # Build training dataloader\n",
    "    train_dataset = MILDataset(df_epoch_train, n_tiles, train_transform, False)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True, num_workers=num_workers)\n",
    "    # train model\n",
    "    train_loss, train_acc, train_auc = train(model, device, train_dataloader, optimizer, epoch)\n",
    "    print(torch.cuda.max_memory_allocated(device=None) / (1024 ** 2))\n",
    "    # Test the model on validation data\n",
    "    print(f'Running {num_vals} tests on validation data.')\n",
    "    losses = []\n",
    "    accs = []\n",
    "    aucs = []\n",
    "    labels = []\n",
    "    \n",
    "    df_trials = pd.DataFrame()\n",
    "    for run in range(num_vals): # Run several validations to account for random tile selections\n",
    "        # Build validation dataloader\n",
    "        val_dataset = MILDataset(df_test_val, n_tiles_val, test_transform, False)    \n",
    "        val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "        val_loss, val_acc, val_auc, val_preds, val_labels, val_probs = validate(model, device, val_dataloader)\n",
    "        losses.append(val_loss)\n",
    "        accs.append(val_acc)\n",
    "        aucs.append(val_auc)\n",
    "        df_trials[run] = val_preds\n",
    "        \n",
    "    avg_val_loss = round(sum(losses) / len(losses), 4)\n",
    "    avg_val_acc = round(sum(accs) / len(accs), 4)\n",
    "    avg_val_auc = round(sum(aucs) / len(aucs), 4)\n",
    "    time.sleep(5)\n",
    "    if epoch < num_epochs:\n",
    "        clear_output(wait=True)\n",
    "    print(f'  Average validation scores: Loss = {avg_val_loss}, Acc = {avg_val_acc}, AUROC = {avg_val_auc}')\n",
    "\n",
    "    # Ensemble prediction\n",
    "    e_acc, e_roc_auc, sens, spec = test_metrics(df_test_val[label].tolist(), df_trials, e_choice, 'validation', val_probs)\n",
    "    print(f'Sensitivity: {round(sens, 3)}, Specificity: {round(spec, 3)}')\n",
    "    improved = 100 * round(e_roc_auc - avg_val_auc,3)\n",
    "    ensemble_improvement.append(improved)\n",
    "    if avg_val_auc >= best_val_auc:\n",
    "        best_val_auc = avg_val_auc\n",
    "        best_val_auc_epoch = epoch\n",
    "        is_best = True\n",
    "        checkpoint = create_checkpoint(epoch, model, optimizer, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, sens, spec, early_stopper)\n",
    "        save_ckp(checkpoint, checkpoint_dir, is_best)\n",
    "    else:\n",
    "         is_best = False   \n",
    "    # print(f'Ensemble Accuracy: {e_acc}, AUROC: {e_roc_auc}, Improved by: {improved} points!')\n",
    "\n",
    "    \n",
    "    avg_e_improv = round(sum(ensemble_improvement) / len(ensemble_improvement),3)\n",
    "    # print(f'Previous Epoch: Ensemble AUROC: {e_roc_auc} versus Average: {avg_val_auc}, Improved by: {improved} points!')\n",
    "\n",
    "    print(f'Best average AUROC: {best_val_auc} at epoch: {best_val_auc_epoch} Average Ensemble Improvement: {avg_e_improv} points')\n",
    "\n",
    "    stats[0].append(train_loss)\n",
    "    stats[1].append(train_acc)\n",
    "    stats[2].append(train_auc)\n",
    "    stats[3].append(avg_val_loss)\n",
    "    stats[4].append(avg_val_acc)\n",
    "    stats[5].append(avg_val_auc)\n",
    "    stats[6].append(e_roc_auc)\n",
    "\n",
    "    # Plot the results\n",
    "    figure, axis = plt.subplots(1, 3)\n",
    "    axis[0].plot(range(epoch), stats[0], label = 'train')\n",
    "    axis[0].plot(range(epoch), stats[3], label = 'val')\n",
    "    axis[0].set_title('Loss')\n",
    "    axis[0].set_xlabel('Epoch')\n",
    "    axis[1].plot(range(epoch), stats[1], label = 'train')\n",
    "    axis[1].plot(range(epoch), stats[4], label = 'val')\n",
    "    axis[1].set_title('Accuracy')\n",
    "    axis[1].set_xlabel('Epoch')\n",
    "    axis[2].plot(range(epoch), stats[2], label = 'train')\n",
    "    axis[2].plot(range(epoch), stats[5], label = 'val')\n",
    "    axis[2].plot(range(epoch), stats[6], label = 'ensemble')\n",
    "    axis[2].set_title('AUROC')\n",
    "    axis[2].set_xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'figures/Test_{model.name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    early_stopper.print_stop_status()\n",
    "\n",
    "    # Save metrics\n",
    "    print(f'Saving metrics...')\n",
    "    checkpoint = create_checkpoint(epoch + 1, model, optimizer, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, sens, spec, early_stopper)\n",
    "    save_ckp(checkpoint, checkpoint_dir, False)\n",
    "    if early_stopper.early_stop(avg_val_loss, train_loss):             \n",
    "        break\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Test the model on test data\n",
    "   \n",
    "df = df_test\n",
    "ckp_path = f\"checkpoints/best_checkpoint-{date}.pt\"\n",
    "model, optimizer, start_epoch, stats, best_val_auc, best_val_auc_epoch, ensemble_improvement, sens, spec, early_stopper = load_ckp(ckp_path, model, optimizer)\n",
    "model.to(device)\n",
    "trail_tile_count = 500\n",
    "base_count = 500\n",
    "blue_color = '\\033[94m'\n",
    "green_code = '\\033[92m'\n",
    "reset_color = '\\033[0m'\n",
    "runs = 5\n",
    "\n",
    "if class1 < class0:\n",
    "    e_choice = 'max'\n",
    "elif class0 < class1:\n",
    "    e_choice = 'min'\n",
    "else:\n",
    "    e_choice = 'avg'\n",
    "\n",
    "# e_choice = 'max'\n",
    "print(f' Utilizing Ensemble with {e_choice} chooser.')\n",
    "\n",
    "print('Initialize Model with Small Bag Size')\n",
    "test_dataset = MILDataset(df, 3, test_transform, False)    \n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "test_loss, base_test_acc, base_test_auc, test_infer, base_test_labels, base_test_probs = validate(model, device, test_dataloader)\n",
    "\n",
    "print('Running Base Test')\n",
    "test_dataset = MILDataset(df, base_count, test_transform, False)    \n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "test_loss, base_test_acc, base_test_auc, test_infer, base_test_labels, base_test_probs = validate(model, device, test_dataloader)\n",
    "\n",
    "\n",
    "stats = [[], [], [], [], [], [], []]\n",
    "for j in range(runs):\n",
    "    num_val = int(base_count / trail_tile_count)\n",
    "    print(f'Running {runs} tests on test data.')\n",
    "    print(f'Ensemble {j+1}/{runs}')\n",
    "    losses = []\n",
    "    accs = []\n",
    "    aucs = []\n",
    "    labels = []\n",
    "    df_trials = pd.DataFrame()\n",
    "    tic = time.time()\n",
    "    for run in range(num_val): # Run several validations to account for random tile selections\n",
    "        # Build validation dataloader\n",
    "        val_dataset = MILDataset(df, trail_tile_count, test_transform, False)    \n",
    "        val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "        val_loss, val_acc, val_auc, df_trials[run], val_labels, val_probs = validate(model, device, val_dataloader)\n",
    "        losses.append(val_loss)\n",
    "        accs.append(val_acc)\n",
    "        aucs.append(val_auc)\n",
    "        \n",
    "    toc = time.time()\n",
    "    \n",
    "    avg_val_acc = round(sum(accs) / len(accs), 4)\n",
    "    avg_val_auc = round(sum(aucs) / len(aucs), 4)\n",
    "\n",
    "    # Ensemble prediction\n",
    "    e_acc, e_roc_auc, sens, spec = test_metrics(df[label].tolist(), df_trials, e_choice, 'test', val_probs)\n",
    "    print(f'Sensitivity: {sens}, Specificity: {spec}')        \n",
    "    stats[0].append(toc - tic)\n",
    "    stats[1].append(e_acc)\n",
    "    stats[2].append(e_roc_auc)\n",
    "    stats[3].append(sens)\n",
    "    stats[4].append(spec)\n",
    "    stats[5].append(e_acc - base_test_acc)\n",
    "    stats[6].append(e_roc_auc - base_test_auc)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f'Base Accuracy: {round(base_test_acc, 3)}')\n",
    "    print(f'Base AUROC:    {round(base_test_auc, 3)}')\n",
    "    print(f'Time to run ensemble with {num_val} tests: {round(stats[0][len(stats[0])-1], 3)} sec')\n",
    "    print(f'Average Accuracy  : {round(sum(accs) / len(accs), 3)}')\n",
    "    print(f'Average AUROC     : {round(sum(aucs) / len(aucs), 3)}')\n",
    "    print(f'Ensemble Accuracy : {round(stats[1][len(stats[1])-1], 3)}')\n",
    "    print(f'Ensemble AUROC    : {round(stats[2][len(stats[2])-1], 3)}')\n",
    "    print(f'Ensemble Sensitiv : {round(stats[3][len(stats[3])-1], 3)}')\n",
    "    print(f'Ensemble Specitiv : {round(stats[4][len(stats[4])-1], 3)}')\n",
    "    print(f'Ensemble Accuracy Improvement: {100 * round(stats[5][len(stats[5])-1], 3)} points')\n",
    "    print(f'Ensemble AUROC    Improvement: {100 * round(stats[6][len(stats[6])-1], 3)} points')\n",
    "    print()\n",
    "\n",
    "avg_acc_imp = sum(stats[5]) / len(stats[5])\n",
    "avg_auc_imp = sum(stats[6]) / len(stats[6])\n",
    "avg_e_acc = sum(stats[1]) / len(stats[1])\n",
    "avg_e_auc = sum(stats[2]) / len(stats[2])\n",
    "print(f'{green_code}')\n",
    "print(f'Average time to run ensemble with {num_val} tests: {round(sum(stats[0]) / len(stats[0]),3)}')\n",
    "print(f'Average snsemble Accuracy: {round(avg_e_acc, 3)}')\n",
    "print(f'Average ensemble AUROC   : {round(avg_e_auc, 3)}')\n",
    "print(f'Average ensemble Sensi   : {round(sum(stats[3]) / len(stats[3]), 3)}')\n",
    "print(f'Average ensemble Speci   : {round(sum(stats[4]) / len(stats[4]), 3)}')\n",
    "print(f'Average Ensemble Accuracy Improvement: {100 * round(avg_acc_imp, 3)} points!')\n",
    "print(f'Average Ensemble AUROC    Improvement: {100 * round(avg_auc_imp, 3)} points!')\n",
    "print(f'{reset_color}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "e-V7ubdYW1Fs",
    "gJOswNVSUn8e",
    "W7XZ0vMTYRuu",
    "4i8st7lonVBT",
    "pCTKUUUUxKDm",
    "XsYUp2QF8RK_",
    "5iDoBFgbm71E",
    "6kEnbSD-m3fQ",
    "Rj7juXaeNTWG",
    "SUxncFoYNJQp",
    "oidOd6iegPx0"
   ],
   "gpuClass": "premium",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python (jupyter)",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
